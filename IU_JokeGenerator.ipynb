{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R8zkd1knF38f",
        "outputId": "d97e7a38-6bd4-4109-85a6-3d639cc5c0c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2023-03-28 22:01:35--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1,1M) [text/plain]\n",
            "Saving to: ‘input.txt.2’\n",
            "\n",
            "input.txt.2         100%[===================>]   1,06M  3,58MB/s    in 0,3s    \n",
            "\n",
            "2023-03-28 22:01:36 (3,58 MB/s) - ‘input.txt.2’ saved [1115394/1115394]\n",
            "\n",
            "Requirement already satisfied: tiktoken in ./venv/lib/python3.11/site-packages (0.3.2)\n",
            "Requirement already satisfied: regex>=2022.1.18 in ./venv/lib/python3.11/site-packages (from tiktoken) (2023.3.23)\n",
            "Requirement already satisfied: requests>=2.26.0 in ./venv/lib/python3.11/site-packages (from tiktoken) (2.28.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in ./venv/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (3.1.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./venv/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (2022.12.7)\n",
            "Requirement already satisfied: torch in ./venv/lib/python3.11/site-packages (2.0.0)\n",
            "Requirement already satisfied: filelock in ./venv/lib/python3.11/site-packages (from torch) (3.10.7)\n",
            "Requirement already satisfied: typing-extensions in ./venv/lib/python3.11/site-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in ./venv/lib/python3.11/site-packages (from torch) (1.11.1)\n",
            "Requirement already satisfied: networkx in ./venv/lib/python3.11/site-packages (from torch) (3.0)\n",
            "Requirement already satisfied: jinja2 in ./venv/lib/python3.11/site-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in ./venv/lib/python3.11/site-packages (from jinja2->torch) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in ./venv/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in ./venv/lib/python3.11/site-packages (1.24.2)\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "!pip install tiktoken\n",
        "!pip install torch\n",
        "!pip install numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "FazteF-ADdxa"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from torch.nn import functional as F\n",
        "\n",
        "\n",
        "# Setting some parameters and loading the dataset\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "with open(\"input.txt\", \"r\") as file:\n",
        "    text = file.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "_jHiQ6ufGHtv"
      },
      "outputs": [],
      "source": [
        "characters = sorted(list(set(text)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NnKKoXS_TiYL",
        "outputId": "82b98fce-c379-4768-d253-ebbafafe0ec3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hello it's me!\n"
          ]
        }
      ],
      "source": [
        "# Just to try it, remove later\n",
        "\n",
        "str_to_int = { s : i for i, s in enumerate(characters) }\n",
        "int_to_str = { i : s for i, s in enumerate(characters) }\n",
        "\n",
        "def encode(text: str):\n",
        "    return [str_to_int[character] for character in text]\n",
        "\n",
        "def decode(encoded_arr: list):\n",
        "    decoded_list = [int_to_str[value] for value in encoded_arr]\n",
        "    return \"\".join(decoded_list)\n",
        "\n",
        "print(decode(encode(\"Hello it's me!\")))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gVNfnUknW3vD"
      },
      "source": [
        "But we will be using a library called tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WqqxhwFhWtFY",
        "outputId": "55f24b20-9fbc-43e4-a68b-231c70e8db32"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "vocab size: 100277\n"
          ]
        }
      ],
      "source": [
        "# tiktoken\n",
        "\n",
        "import tiktoken\n",
        "\n",
        "# enc = tiktoken.encoding_for_model(\"gpt-4\")\n",
        "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
        "\n",
        "test_text = \"Hello, it's me!\"\n",
        "# encoded_data = enc.encode(test_text)\n",
        "encoded_data = enc.encode(text)\n",
        "vocab_size = enc.n_vocab\n",
        "embed_dims = 32\n",
        "\n",
        "# print(encoded_data)\n",
        "# print([enc.decode([token]) for token in encoded_data])\n",
        "print(f\"vocab size: {vocab_size}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7EYWjdUf-Js"
      },
      "source": [
        "We will be processing several batches in parallel to accelerate training process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "oSvw-mDXmBTc"
      },
      "outputs": [],
      "source": [
        "# JGM stands for Joke Generation Model\n",
        "class JGM(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.block_size = 32\n",
        "        self.batch_size = 4\n",
        "\n",
        "        self.tok_emb_table = nn.Embedding(vocab_size, embed_dims)\n",
        "        self.pos_emb_table = nn.Embedding(self.block_size, embed_dims)\n",
        "        self.lm_head = nn.Linear(embed_dims, vocab_size)\n",
        "    \n",
        "    def get_batch(self, data):\n",
        "        ix = torch.randint(len(data) - self.block_size, (self.batch_size,))\n",
        "        x = torch.stack([data[i : i + self.block_size] for i in ix])\n",
        "        y = torch.stack([data[i + 1 : i + self.block_size + 1] for i in ix])\n",
        "        return x, y\n",
        "    \n",
        "    def forward(self, xs, ys=None):\n",
        "        batches, positions = xs.shape\n",
        "\n",
        "        tok_emb = self.tok_emb_table(xs)\n",
        "        pos_emb = self.pos_emb_table(torch.arange(positions, device=device))\n",
        "\n",
        "        composed = tok_emb + pos_emb\n",
        "        logits = self.lm_head(composed)\n",
        "\n",
        "        if ys is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "\n",
        "            logits = logits.view(B * T, C)\n",
        "            ys = ys.view(B * T)\n",
        "            loss = F.cross_entropy(logits, ys)\n",
        "\n",
        "        return logits, loss\n",
        "    \n",
        "    def generate(self, xs, max_new_tokens):\n",
        "\n",
        "        for _ in range(max_new_tokens):\n",
        "\n",
        "            logits, loss = self(xs)\n",
        "\n",
        "            logits = logits[:, -1, :] # (B, C)\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            idx = torch.cat((xs, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VaJV4kMBBwbm"
      },
      "outputs": [],
      "source": [
        "m = JGM(vocab_size)\n",
        "\n",
        "xb, yb = m.get_batch(encoded_data)\n",
        "\n",
        "logits, loss = m(xb, yb)\n",
        "print(logits.shape)\n",
        "print(loss)\n",
        "\n",
        "# print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=5)[0].tolist()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nGNIZyyqNBDG"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RbU357BaNKrO"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "steps = 1000\n",
        "\n",
        "for step in range(steps):\n",
        "    xb, yb = m.get_batch(encoded_data)\n",
        "\n",
        "    logits, loss = m(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "print(f\"loss: {loss.item()}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
