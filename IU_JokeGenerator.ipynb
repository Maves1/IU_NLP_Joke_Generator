{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "!pip install tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R8zkd1knF38f",
        "outputId": "d97e7a38-6bd4-4109-85a6-3d639cc5c0c2"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-03-27 16:37:53--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.03s   \n",
            "\n",
            "2023-03-27 16:37:53 (36.8 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.3.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.9/dist-packages (from tiktoken) (2.27.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.9/dist-packages (from tiktoken) (2022.10.31)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.26.0->tiktoken) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.26.0->tiktoken) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests>=2.26.0->tiktoken) (2.0.12)\n",
            "Installing collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "FazteF-ADdxa"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from torch.nn import functional as F\n",
        "\n",
        "\n",
        "# Setting some parameters and loading the dataset\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "with open(\"input.txt\", \"r\") as file:\n",
        "    text = file.read()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "characters = sorted(list(set(text)))"
      ],
      "metadata": {
        "id": "_jHiQ6ufGHtv"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Just to try it, remove later\n",
        "\n",
        "str_to_int = { s : i for i, s in enumerate(characters) }\n",
        "int_to_str = { i : s for i, s in enumerate(characters) }\n",
        "\n",
        "def encode(text: str):\n",
        "    return [str_to_int[character] for character in text]\n",
        "\n",
        "def decode(encoded_arr: list):\n",
        "    decoded_list = [int_to_str[value] for value in encoded_arr]\n",
        "    return \"\".join(decoded_list)\n",
        "\n",
        "print(decode(encode(\"Hello it's me!\")))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NnKKoXS_TiYL",
        "outputId": "82b98fce-c379-4768-d253-ebbafafe0ec3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello it's me!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "But we will be using a library called tiktoken"
      ],
      "metadata": {
        "id": "gVNfnUknW3vD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# tiktoken\n",
        "\n",
        "import tiktoken\n",
        "\n",
        "# enc = tiktoken.encoding_for_model(\"gpt-4\")\n",
        "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
        "\n",
        "test_text = \"Hello, it's me!\"\n",
        "encoded_data = enc.encode(test_text)\n",
        "vocab_size = enc.n_vocab\n",
        "\n",
        "print(encoded_data)\n",
        "print([enc.decode([token]) for token in encoded_data])\n",
        "print(f\"vocab size: {vocab_size}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WqqxhwFhWtFY",
        "outputId": "55f24b20-9fbc-43e4-a68b-231c70e8db32"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[9906, 11, 433, 596, 757, 0]\n",
            "['Hello', ',', ' it', \"'s\", ' me', '!']\n",
            "vocab size: 100277\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will be processing several batches in parallel to accelerate training process"
      ],
      "metadata": {
        "id": "-7EYWjdUf-Js"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# JGM stands for Joke Generation Model\n",
        "class JGM(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "\n",
        "        self.block_size = 8\n",
        "        self.batch_size = 4\n",
        "\n",
        "        self.tok_emb_table = nn.Embedding(vocab_size, vocab_size)\n",
        "    \n",
        "    def get_batch(self, data):\n",
        "        ix = torch.randint(len(data) - self.block_size, (self.batch_size,))\n",
        "        x = torch.stack([data[i : i + self.block_size] for i in ix])\n",
        "        y = torch.stack([data[i + 1 : i + self.block_size + 1] for i in ix])\n",
        "        return x, y\n",
        "    \n",
        "    def forward(self, xs, ys):\n",
        "        preds = self.tok_emb_table(xs)\n",
        "\n",
        "        B, T, C = preds.shape\n",
        "\n",
        "        preds = preds.view(B * T, C)\n",
        "        ys = ys.view(B * T)\n",
        "        loss = F.cross_entropy(preds, ys)\n",
        "\n",
        "        return preds, loss\n",
        "    \n",
        "    def generate(self, xs, max_new_tokens):\n",
        "\n",
        "        for _ in range(max_new_tokens):\n",
        "\n",
        "            logits, loss = self(xs)\n",
        "\n",
        "            logits = logits[:, -1, :] # (B, C)\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            idx = torch.cat((xs, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n"
      ],
      "metadata": {
        "id": "oSvw-mDXmBTc"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m = JGM(vocab_size)\n",
        "\n",
        "xb, yb = m.get_batch(encoded_data)\n",
        "\n",
        "logits, loss = m(xb, yb)\n",
        "print(logits.shape)\n",
        "print(loss)\n",
        "\n",
        "# print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=5)[0].tolist()))"
      ],
      "metadata": {
        "id": "VaJV4kMBBwbm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "nGNIZyyqNBDG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "steps = 1000\n",
        "\n",
        "for step in range(steps):\n",
        "    xb, yb = m.get_batch(encoded_data)\n",
        "\n",
        "    logits, loss = m(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "print(f\"loss: {loss.item()}\")"
      ],
      "metadata": {
        "id": "RbU357BaNKrO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}