{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R8zkd1knF38f",
        "outputId": "d97e7a38-6bd4-4109-85a6-3d639cc5c0c2"
      },
      "outputs": [],
      "source": [
        "!pip install tiktoken\n",
        "!pip install torch\n",
        "!pip install numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "FazteF-ADdxa"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from torch.nn import functional as F\n",
        "\n",
        "\n",
        "# Setting some parameters and loading the dataset\n",
        "\n",
        "device = \"mps\"\n",
        "\n",
        "with open(\"input.txt\", \"r\") as file:\n",
        "    text = file.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "_jHiQ6ufGHtv"
      },
      "outputs": [],
      "source": [
        "characters = sorted(list(set(text)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NnKKoXS_TiYL",
        "outputId": "82b98fce-c379-4768-d253-ebbafafe0ec3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hello it's me!\n"
          ]
        }
      ],
      "source": [
        "# We will be using characters as tokens\n",
        "\n",
        "str_to_int = { s : i for i, s in enumerate(characters) }\n",
        "int_to_str = { i : s for i, s in enumerate(characters) }\n",
        "\n",
        "vocab_size = len(characters)\n",
        "\n",
        "def encode(text: str):\n",
        "    return [str_to_int[character] for character in text]\n",
        "\n",
        "def decode(encoded_arr: list):\n",
        "    decoded_list = [int_to_str[value] for value in encoded_arr]\n",
        "    return \"\".join(decoded_list)\n",
        "\n",
        "dataset = encode(text)\n",
        "\n",
        "train_percentage = 0.85\n",
        "\n",
        "train_data = torch.tensor(dataset[:int(train_percentage * len(dataset))]).to(device)\n",
        "test_data  = torch.tensor(dataset[int(train_percentage * len(dataset)):]).to(device)\n",
        "\n",
        "print(decode(encode(\"Hello it's me!\")))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "gVNfnUknW3vD"
      },
      "source": [
        "But we will be using a library called tiktoken (upd: for now we won't, but that might change)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WqqxhwFhWtFY",
        "outputId": "55f24b20-9fbc-43e4-a68b-231c70e8db32"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "vocab size: 50257\n"
          ]
        }
      ],
      "source": [
        "# tiktoken\n",
        "\n",
        "# import tiktoken\n",
        "\n",
        "# # enc = tiktoken.encoding_for_model(\"gpt-4\")\n",
        "# enc = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "# test_text = \"Hello, it's me!\"\n",
        "# # encoded_data = enc.encode(test_text)\n",
        "# encoded_data = enc.encode(text)\n",
        "# vocab_size = enc.n_vocab\n",
        "# n_embed_dims = 32\n",
        "\n",
        "# # print(encoded_data)\n",
        "# # print([enc.decode([token]) for token in encoded_data])\n",
        "# print(f\"vocab size: {vocab_size}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7EYWjdUf-Js"
      },
      "source": [
        "We will be processing several batches in parallel to accelerate training process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "block_size = 8\n",
        "n_embed_dims = 4\n",
        "\n",
        "max_iterations = 2000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "oSvw-mDXmBTc"
      },
      "outputs": [],
      "source": [
        "# JGM stands for Joke Generation Model\n",
        "class JGM(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.tok_emb_table = nn.Embedding(vocab_size, n_embed_dims)\n",
        "        self.pos_emb_table = nn.Embedding(block_size, n_embed_dims)\n",
        "        self.lm_head = nn.Linear(n_embed_dims, vocab_size)\n",
        "    \n",
        "    def get_batch(self, data):\n",
        "        offsets = torch.randint(len(data) - block_size, (batch_size,))\n",
        "        x = torch.stack([data[i : i + block_size] for i in offsets]).to(device)\n",
        "        y = torch.stack([data[i + 1 : i + block_size + 1] for i in offsets]).to(device)\n",
        "        return x, y\n",
        "    \n",
        "    def forward(self, xs, ys=None):\n",
        "        batches, positions = xs.shape\n",
        "\n",
        "        tok_emb = self.tok_emb_table(xs)\n",
        "        pos_emb = self.pos_emb_table(torch.arange(positions, device=device))\n",
        "\n",
        "        composed = tok_emb + pos_emb\n",
        "        logits = self.lm_head(composed)\n",
        "\n",
        "        if ys is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "\n",
        "            logits = logits.view(B * T, C)\n",
        "            ys = ys.view(B * T)\n",
        "            loss = F.cross_entropy(logits, ys)\n",
        "\n",
        "        return logits, loss\n",
        "    \n",
        "    def generate(self, xs, max_new_tokens):\n",
        "\n",
        "        for _ in range(max_new_tokens):\n",
        "            logits, loss = self(xs)\n",
        "\n",
        "            logits = logits[:, -1, :] # (B, C)\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            next_pred = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            xs = torch.cat((xs, next_pred), dim=1)\n",
        "        return xs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "VaJV4kMBBwbm"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([256, 65])\n",
            "tensor(4.6050, device='mps:0', grad_fn=<NllLossBackward0>)\n"
          ]
        }
      ],
      "source": [
        "m = JGM()\n",
        "m.to(device)\n",
        "\n",
        "xb, yb = m.get_batch(train_data)\n",
        "\n",
        "logits, loss = m(xb, yb)\n",
        "print(logits.shape)\n",
        "print(loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "vG-3RRNH nUE w;YbNM'phVnTD?EZk\n",
            "?,IlgTP.UoKqtvOtQEr\n"
          ]
        }
      ],
      "source": [
        "print(decode(m.generate(xs = torch.zeros((1, 1), dtype=torch.long, device=device), max_new_tokens=50)[0].tolist()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "nGNIZyyqNBDG"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "RbU357BaNKrO"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 10000/10000 [00:42<00:00, 233.21it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss: 2.5588533878326416\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import tqdm\n",
        "\n",
        "batch_size = 32\n",
        "steps = 10000\n",
        "\n",
        "for step in tqdm.tqdm(range(steps)):\n",
        "    xb, yb = m.get_batch(train_data)\n",
        "\n",
        "    logits, loss = m(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "print(f\"loss: {loss.item()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Whid ofagoXIOChoflathatalLIYandanthonuvaconuroREUSESAROLELEESASGMESSEREEREMDUESEDESSILECIKUWAEOERHOn\n"
          ]
        }
      ],
      "source": [
        "print(decode(m.generate(xs = torch.zeros((1, 1), dtype=torch.long, device=device), max_new_tokens=100)[0].tolist()))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
